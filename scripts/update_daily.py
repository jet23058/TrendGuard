#!/usr/bin/env python3
"""
ç¯©é¸æ¢ä»¶ï¼š
1. è‚¡åƒ¹ç«™ä¸Šæ‰€æœ‰å‡ç·š (MA5, MA10, MA20, MA60)
2. é€£çºŒå…©æ—¥ç´… K (æ”¶ç›¤ > é–‹ç›¤)
3. æ”¶ç›¤åƒ¹çªç ´è¿‘ N æ—¥æ–°é«˜
4. (ä¿®æ­£) è­¦ç¤º/è™•ç½®è‚¡ä¹Ÿå¿…é ˆç¬¦åˆä¸Šè¿°æŠ€è¡“æ¢ä»¶æ‰èƒ½å…¥é¸
"""
import json
import os
import sys
import concurrent.futures
import time
import threading
from datetime import datetime
from pathlib import Path

# Add parent directory to path for imports
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

import pandas as pd

# Stock Data Provider (é¸æ“‡ä½¿ç”¨ TWSE æˆ– FinMind)
USE_FACADE = os.environ.get('USE_STOCK_FACADE', 'true').lower() == 'true'

if USE_FACADE:
    # Use new Facade pattern for flexible data source
    from stock_facade_adapter import FacadeDataLoader as DataLoader, get_stock_facade
    _finmind_loader = None
    
    def get_finmind_loader():
        """Get or create Stock Data Facade singleton"""
        global _finmind_loader
        if _finmind_loader is None:
            _finmind_loader = DataLoader()
            token = os.environ.get("FINMIND_API_TOKEN")
            if token:
                _finmind_loader.login_by_token(api_token=token)
        return _finmind_loader
else:
    # Traditional FinMind API for Taiwan stock data
    from FinMind.data import DataLoader
    
    # Initialize FinMind DataLoader (singleton)
    _finmind_loader = None
    
    def get_finmind_loader():
        """Get or create FinMind DataLoader singleton"""
        global _finmind_loader
        if _finmind_loader is None:
            _finmind_loader = DataLoader()
            token = os.environ.get("FINMIND_API_TOKEN")
            if token:
                _finmind_loader.login_by_token(api_token=token)
                print("âœ… FinMind logged in with token")
        return _finmind_loader

try:
    import twstock
    # å¼·åˆ¶æ›´æ–°è‚¡ç¥¨ä»£ç¢¼è¡¨ï¼Œç¢ºä¿æ“æœ‰æœ€æ–°ä¸Šå¸‚æ«ƒæ¸…å–®
    # twstock.__update_codes() # æ³¨æ„: é€™å¯èƒ½éœ€è¦ä¸‹è¼‰ï¼Œè‹¥ CI ç’°å¢ƒå—é™å¯èƒ½å¤±æ•—ï¼Œè¦–æƒ…æ³å•Ÿç”¨
    HAS_TWSTOCK = True
except ImportError:
    HAS_TWSTOCK = False
    print("Warning: twstock not installed, using FinMind for stock names")

import re
import requests
from datetime import timedelta





def roc_to_date(roc_str):
    """Convert ROC date string (e.g., '114/01/05') to datetime object"""
    try:
        parts = roc_str.split('/')
        year = int(parts[0]) + 1911
        return datetime(year, int(parts[1]), int(parts[2]))
    except:
        return None

def fetch_market_alerts():
    """Fetch TWSE/TPEx Warning and Disposition data with Risk Analysis"""
    alerts = {}
    history_db = {} # {code: [dates]}
    
    today = datetime.now()
    today_str = today.strftime('%Y%m%d')
    # Look back 40 days to ensure we have enough trading days for the 30-day rule
    start_str = (today - timedelta(days=40)).strftime('%Y%m%d') 
    
    # 1. Fetch TWSE Warning History (Notice)
    try:
        url = "https://www.twse.com.tw/rwd/zh/announcement/notice"
        params = {'response': 'json', 'startDate': start_str, 'endDate': today_str}
        r = requests.get(url, params=params, timeout=10)
        data = r.json()
        
        if 'data' in data:
            for item in data['data']:
                code = item[1]
                date_roc = item[5] # e.g. "114/01/30"
                reason = item[4]
                
                if code not in history_db:
                    history_db[code] = []
                history_db[code].append(date_roc)
                
                # If it's today's alert, initialize the alert object
                alert_dt = roc_to_date(date_roc)
                if alert_dt and (today - alert_dt).days <= 1:
                    alerts[code] = {
                        "type": "warning",
                        "badge": "è­¦ç¤º",
                        "color": "yellow",
                        "info": "æ³¨æ„è‚¡",
                        "detail": reason,
                        "history": []
                    }
    except Exception as e:
        print(f"Error fetching TWSE notice: {e}")

    # 2. Fetch TWSE Disposition (Punish)
    try:
        url = "https://www.twse.com.tw/rwd/zh/announcement/punish"
        params = {'response': 'json', 'startDate': start_str, 'endDate': today_str}
        r = requests.get(url, params=params, timeout=10)
        data = r.json()
        
        if 'data' in data:
            for item in data['data']:
                code = item[2]
                period_str = item[6] 
                content = item[8]
                
                if 'ï½' in period_str:
                    start_roc, end_roc = period_str.split('ï½')
                    start_dt = roc_to_date(start_roc)
                    end_dt = roc_to_date(end_roc)
                    
                    if start_dt and end_dt and start_dt <= today + timedelta(days=1) and today <= end_dt + timedelta(days=1): 
                        freq = "è™•ç½®"
                        match = re.search(r'æ¯(\S+)åˆ†é˜', content)
                        if match:
                            freq = f"{match.group(1)}åˆ†ç›¤"
                        elif "äººå·¥ç®¡åˆ¶" in content:
                            freq = "äººå·¥ç®¡åˆ¶"
                            
                        alerts[code] = {
                            "type": "disposition",
                            "badge": "è™•ç½®",
                            "color": "red",
                            "info": f"{freq} (è‡³ {end_roc})",
                            "detail": f"æœŸé–“: {period_str}\næªæ–½: {item[7]}",
                            "is_disposed": True
                        }
    except Exception as e:
        print(f"Error fetching TWSE punish: {e}")

    # 3. Fetch TPEX (OTC) Alerts
    try:
        base_url = "https://www.tpex.org.tw/openapi/v1"
        
        # 3.1 TPEX Warning History
        r = requests.get(f"{base_url}/tpex_trading_warning_information", timeout=10)
        if r.status_code == 200:
            for item in r.json():
                code = item.get('SecuritiesCompanyCode')
                date_roc = item.get('Date') # Compact "1140130"
                reason = item.get('TradingInformation', '')
                
                # Convert to slash format for consistency
                if len(date_roc) == 7:
                    fmt_date = f"{date_roc[:3]}/{date_roc[3:5]}/{date_roc[5:]}"
                else:
                    fmt_date = date_roc
                    
                if code not in history_db:
                    history_db[code] = []
                history_db[code].append(fmt_date)
                
                # Check if recent
                alert_dt = None
                if len(date_roc) == 7:
                    alert_dt = datetime(int(date_roc[:3]) + 1911, int(date_roc[3:5]), int(date_roc[5:]))
                
                if alert_dt and (today - alert_dt).days <= 1:
                    if code not in alerts:
                        alerts[code] = {
                            "type": "warning",
                            "badge": "è­¦ç¤º",
                            "color": "yellow",
                            "info": "æ³¨æ„è‚¡",
                            "detail": reason,
                            "history": []
                        }

        # 3.2 TPEX Disposition
        r = requests.get(f"{base_url}/tpex_disposal_information", timeout=10)
        if r.status_code == 200:
            for item in r.json():
                code = item.get('SecuritiesCompanyCode')
                period_str = item.get('DispositionPeriod', '')
                content = item.get('DisposalCondition', '')
                
                if '~' in period_str:
                    try:
                        start_roc, end_roc = period_str.split('~')
                        def parse_roc_compact(d_str):
                            if len(d_str) == 7:
                                return datetime(int(d_str[:3]) + 1911, int(d_str[3:5]), int(d_str[5:]))
                            return roc_to_date(d_str)
                            
                        start_dt = parse_roc_compact(start_roc)
                        end_dt = parse_roc_compact(end_roc)
                        
                        if start_dt and end_dt and start_dt <= today + timedelta(days=1) and today <= end_dt + timedelta(days=1):
                            freq = "è™•ç½®"
                            match = re.search(r'æ¯(\S+)åˆ†é˜', content)
                            if match:
                                freq = f"{match.group(1)}åˆ†ç›¤"
                            elif "äººå·¥ç®¡åˆ¶" in content:
                                freq = "äººå·¥ç®¡åˆ¶"
                                
                            alerts[code] = {
                                "type": "disposition",
                                "badge": "è™•ç½®",
                                "color": "red",
                                "info": f"{freq} (è‡³ {end_roc})",
                                "detail": f"æœŸé–“: {period_str}\næªæ–½: {content}",
                                "is_disposed": True
                            }
                    except: pass
    except Exception as e:
        print(f"Error fetching TPEX alerts: {e}")

    # 4. Risk Analysis (Calculating Disposition Proximity)
    for code, alert_obj in alerts.items():
        if alert_obj.get('type') == 'disposition':
            continue # Already disposed
            
        hist = sorted(list(set(history_db.get(code, []))), reverse=True)
        alert_obj['history'] = hist
        
        # Calculate Risk Metrics
        # Rule 1: 3 consecutive days
        consecutive = 0
        # This is tricky because we need trading days. 
        # For simplicity, we count consecutive entries in the sorted history.
        consecutive = 1
        # Check if previous days were also in history
        # (Need a calendar or just check if dates are close)
        # For now, let's just use the count in recent window.
        
        count_30 = len(hist)
        count_6 = 0
        # Approx count in last 6 trading days (approx 10 calendar days)
        cutoff_6 = today - timedelta(days=10)
        for d_str in hist:
            d_dt = roc_to_date(d_str)
            if d_dt and d_dt >= cutoff_6:
                count_6 += 1
        
        # Determine Risk Level
        risk_level = "low"
        risk_msg = ""
        
        if count_6 >= 3:
            risk_level = "high"
            risk_msg = f"è§¸ç™¼ 4/6 è™•ç½®é¢¨éšª (ç›®å‰ {count_6}/6)"
        elif count_6 >= 2:
            risk_level = "medium"
            risk_msg = f"è¿‘æœŸæ³¨æ„æ¬¡æ•¸å¢åŠ  ({count_6}/6)"
            
        if count_30 >= 10:
            risk_level = "high"
            risk_msg = f"è§¸ç™¼ 12/30 è™•ç½®é¢¨éšª (ç›®å‰ {count_30}/30)"
            
        alert_obj['risk'] = {
            "level": risk_level,
            "message": risk_msg,
            "count_6": count_6,
            "count_30": count_30
        }

    return alerts

def fetch_allowed_day_trade_targets():
    """å–å¾—æ‰€æœ‰å¯ç¾è‚¡ç•¶æ²–çš„è‚¡ç¥¨ä»£ç¢¼ (ä¸Šå¸‚+ä¸Šæ«ƒ)"""
    allowed = set()
    
    # 1. TWSE (ä¸Šå¸‚)
    try:
        # TWTB4U: ç•¶æ—¥æ²–éŠ·äº¤æ˜“æ¨™çš„åŠæˆäº¤é‡å€¼
        # è‹¥ä¸å¸¶æ—¥æœŸï¼Œé è¨­å›å‚³æœ€è¿‘äº¤æ˜“æ—¥
        url = "https://www.twse.com.tw/exchangeReport/TWTB4U?response=json"
        r = requests.get(url, timeout=10)
        data = r.json()
        if 'tables' in data:
            for t in data['tables']:
                # å°‹æ‰¾åŒ…å«æ¨™çš„æ¸…å–®çš„è¡¨æ ¼ (é€šå¸¸æ˜¯ç¬¬äºŒå€‹ï¼Œæ¬„ä½å«'è­‰åˆ¸ä»£è™Ÿ')
                if 'fields' in t and 'è­‰åˆ¸ä»£è™Ÿ' in t['fields']:
                    if len(t.get('data', [])) > 100: # ç°¡å–®æª¢æ ¸è³‡æ–™é‡
                        for row in t['data']:
                            allowed.add(row[0]) # ä»£è™Ÿ
                        print(f"å·²å–å¾—ä¸Šå¸‚ç•¶æ²–æ¨™çš„: {len(t['data'])} æª”")
    except Exception as e:
        print(f"Error fetching TWSE day trade list: {e}")

    # 2. TPEX (ä¸Šæ«ƒ)
    try:
        # TPEX éœ€è¦æŒ‡å®šæ—¥æœŸï¼Œå˜—è©¦å›æ¨æœ€è¿‘ 5 å¤©ç›´åˆ°æŠ“åˆ°è³‡æ–™
        found = False
        base_date = datetime.now()
        
        for i in range(5):
            d = base_date - timedelta(days=i)
            roc_year = d.year - 1911
            date_str = f"{roc_year}/{d.month:02d}/{d.day:02d}"
            
            url = f"https://www.tpex.org.tw/web/stock/trading/intraday_stat/intraday_trading_stat_result.php?l=zh-tw&o=json&d={date_str}"
            try:
                r = requests.get(url, timeout=5)
                data = r.json()
                if 'tables' in data:
                    for t in data['tables']:
                         if 'fields' in t and 'è­‰åˆ¸ä»£è™Ÿ' in t['fields']:
                            count = len(t.get('data', []))
                            if count > 50: # ç°¡å–®æª¢æ ¸
                                for row in t['data']:
                                    allowed.add(row[0])
                                print(f"å·²å–å¾—ä¸Šæ«ƒç•¶æ²–æ¨™çš„ ({date_str}): {count} æª”")
                                found = True
                                break
                    if found: break
            except:
                continue
                
    except Exception as e:
        print(f"Error fetching TPEX day trade list: {e}")
        
    print(f"ç¸½è¨ˆå¯ç•¶æ²–æ¨™çš„: {len(allowed)} æª”")
    return allowed

# --- è¨­å®š ---
LOOKBACK_DAYS = 20  # çªç ´å¹¾æ—¥æ–°é«˜
TEST_MODE = os.environ.get('TEST_MODE', 'true').lower() == 'true'  # GitHub Actions è¨­ç‚º false
OUTPUT_DIR = Path("frontend/public/data")
MAX_WORKERS = int(os.environ.get('MAX_WORKERS', 5)) # Parallel workers

# æ¸¬è©¦ç”¨è‚¡ç¥¨æ¸…å–® (æ“´å¤§ç¯„åœ)
TEST_STOCKS = [
    # æ¬Šå€¼è‚¡
    '2330', '2317', '2454', '2303', '2308', '2412', '2882', '2881', '2886', '2891',
    # èˆªé‹è‚¡
    '2603', '2609', '2615', '2618',
    # AI/åŠå°é«”
    '3035', '6770', '6443', '3037', '3008', '3034', '2379', '3443', '6669',
    # é›»å­ä»£å·¥
    '3231', '2382', '2356', '4938', '2324', '2353',
    # é‡‘èè‚¡
    '2884', '2885', '2887', '2880', '2883',
    # å…¶ä»–ç†±é–€
    '2002', '1301', '1303', '2912', '9910', '2377', '3017', '2327', '6446', '3533',
    # 01/02 ä½¿ç”¨è€…æä¾›æ¸…å–® (ç”¨æ–¼ç¢ºä¿é€£çºŒæ€§/å‰”é™¤åˆ¤å®šæº–ç¢º)
    '3455', '3516', '8064', '3481', '3289', '3402', '3580', '5452', '8431', '5351', 
    '2330', '2337', '2449', '2454', '3006', '3711', '4967', '6531', '8110', '5263', 
    '1460', '8423', '8438', '5704', '3163', '2025', '3360', '6265', '3624', '3689', 
    '2460', '2467', '3092', '3308', '4912', '5288', '5289', '2399',
    # æ¸¬è©¦ç”¨: å—äºç§‘ (è‹¥ä¸åœ¨ä¸Šè¿°æ¸…å–®ä¸­)
    '2408'
]

from typing import Optional

def get_stock_name(code: str) -> tuple:
    """å–å¾—è‚¡ç¥¨ä¸­æ–‡åç¨±ã€ç”¢æ¥­åˆ¥èˆ‡å¸‚å ´åˆ¥"""
    market = "ä¸Šå¸‚" # Default
    
    if HAS_TWSTOCK and code in twstock.codes:
        info = twstock.codes[code]
        market = info.market
        return info.name, info.group if hasattr(info, 'group') else "å…¶ä»–", market
    
    # Fallback: ä½¿ç”¨ FinMind
    try:
        loader = get_finmind_loader()
        df = loader.TaiwanStockInfo()
        stock_info = df[df['stock_id'] == code]
        if not stock_info.empty:
            # FinMind doesn't explicitly separate Listed/OTC in simple info sometimes, 
            # but usually it's there. For now default to 'ä¸Šå¸‚' or guess.
            return stock_info.iloc[0]['stock_name'], stock_info.iloc[0]['industry_category'], market
    except Exception:
        pass
    return code, "å…¶ä»–", market


def get_all_tw_targets() -> list:
    """å–å¾—è¦æƒæçš„è‚¡ç¥¨æ¸…å–®"""
    if TEST_MODE:
        # å»é™¤é‡è¤‡çš„è‚¡ç¥¨ä»£ç¢¼
        unique_stocks = sorted(list(set(TEST_STOCKS)))
        print(f"[æ¸¬è©¦æ¨¡å¼] åƒ…æƒæ {len(unique_stocks)} æª”æ¸¬è©¦è‚¡ç¥¨...")
        return unique_stocks
    
    # å®Œæ•´æƒææ¨¡å¼
    if not HAS_TWSTOCK:
        print("twstock æœªå®‰è£ï¼Œä½¿ç”¨æ¸¬è©¦æ¸…å–®")
        return TEST_STOCKS
    
    targets = []
    print("æ­£åœ¨æ•´ç†å°è‚¡æ¸…å–® (å«è‚¡ç¥¨èˆ‡å•†å“å‹ ETF)...")
    for code, info in twstock.codes.items():
        if info.market in ["ä¸Šå¸‚", "ä¸Šæ«ƒ"]:
            if info.type == "è‚¡ç¥¨" or info.type == "ETF":
                targets.append(code)
    
    print(f"å…± {len(targets)} æª”æ¨™çš„å¾…æƒæ")
    return targets


def check_livermore_criteria(code: str, market_alerts: Optional[dict] = None, allowed_day_trade_targets: Optional[set] = None) -> tuple[Optional[dict], Optional[float]]:
    """
    æª¢æŸ¥æ˜¯å¦ç¬¦åˆåˆ©å¼—æ‘©çˆ¾çªç ´æ¢ä»¶
    
    Returns:
        (full_data, change_pct)
        - full_data: ç¬¦åˆæ¢ä»¶çš„å®Œæ•´è³‡æ–™ï¼Œè‹¥ä¸ç¬¦åˆå‰‡ç‚º None
        - change_pct: è©²è‚¡ç¥¨çš„æ¼²è·Œå¹… (float)ï¼Œè‹¥ç„¡æ³•å–å¾—è³‡æ–™å‰‡ç‚º None
    """
    try:
        # Check alerts first
        alert_data = market_alerts.get(code) if market_alerts else None
        
        # ä½¿ç”¨ FinMind API å–å¾—è‚¡ç¥¨è³‡æ–™
        loader = get_finmind_loader()
        end_date = datetime.now().strftime('%Y-%m-%d')
        
        # æ ¹æ“š Provider å‹•æ…‹èª¿æ•´æ­·å²è³‡æ–™å¤©æ•¸
        # TWSE Provider éœ€è¦é€æœˆè«‹æ±‚ï¼Œæ‰€ä»¥ç¸®çŸ­å¤©æ•¸é¿å…å¤ªå¤šè«‹æ±‚
        # FinMind Provider ä¸€æ¬¡è«‹æ±‚å³å¯ï¼Œæ‰€ä»¥å¯ä»¥æ‹¿è¼ƒå¤šè³‡æ–™
        if USE_FACADE:
            # ä½¿ç”¨ Facade æ™‚ï¼ŒæŸ¥è©¢ provider é¡å‹
            facade = get_stock_facade()
            if facade.get_provider_name() == 'twse':
                # TWSE: åªæŠ“ 60 å¤©ï¼ˆç´„ 3 å€‹æœˆï¼‰ï¼Œæ¸›å°‘ API è«‹æ±‚
                lookback_days = 60
            else:
                # FinMind: æŠ“ 180 å¤©ï¼ˆç´„ 6 å€‹æœˆï¼‰
                lookback_days = 180
        else:
            # å‚³çµ± FinMind: æŠ“ 180 å¤©
            lookback_days = 180
        
        start_date = (datetime.now() - timedelta(days=lookback_days)).strftime('%Y-%m-%d')
        
        raw_df = loader.taiwan_stock_daily(
            stock_id=code,
            start_date=start_date,
            end_date=end_date
        )
        
        if raw_df is None or len(raw_df) < LOOKBACK_DAYS + 2:
            return None, None
        
        # FinMind è¿”å›çš„æ¬„ä½åç¨±èˆ‡ yfinance ä¸åŒï¼Œéœ€è¦è½‰æ›
        # FinMind: date, stock_id, Trading_Volume, Trading_money, open, max, min, close, spread, Trading_turnover
        df = raw_df.copy()
        df = df.rename(columns={
            'open': 'Open',
            'max': 'High',
            'min': 'Low',
            'close': 'Close',
            'Trading_Volume': 'Volume'
        })
        df['date'] = pd.to_datetime(df['date'])
        df = df.set_index('date').sort_index()
        
        # è¨ˆç®—å‡ç·š
        df['MA5'] = df['Close'].rolling(window=5).mean()
        df['MA10'] = df['Close'].rolling(window=10).mean()
        df['MA20'] = df['Close'].rolling(window=20).mean()
        df['MA60'] = df['Close'].rolling(window=60).mean()
        
        today = df.iloc[-1]
        yesterday = df.iloc[-2]
        
        current_price = float(today['Close'])
        prev_close = float(yesterday['Close'])

        # [DEBUG] æ–°å¢ï¼šå°å‡ºæ¯æª”è‚¡ç¥¨çš„æƒæç‹€æ…‹ï¼Œä¾¿æ–¼é™¤éŒ¯
        print(f"[DEBUG] {code} Date:{df.index[-1].strftime('%Y-%m-%d')} Close:{current_price} Open:{today['Open']} Vol:{today['Volume']}")
        
        # æ¼²è·Œå¹… (å³ä¾¿ä¸ç¬¦åˆæ¢ä»¶ä¹Ÿè¦å›å‚³ï¼Œç”¨æ–¼å¸‚å ´çµ±è¨ˆ)
        change_pct = ((current_price - prev_close) / prev_close) * 100
        
        open_price = float(today['Open'])
        
        # è¨ˆç®—è¿‘ N æ—¥æœ€é«˜åƒ¹ (ä¸å«ä»Šæ—¥)
        past_data = df['High'].iloc[-(LOOKBACK_DAYS+1):-1]
        prev_high = float(past_data.max())
        
        # è¨ˆç®—é€£çºŒç´… K å¤©æ•¸
        # ä¿®æ­£: æ’é™¤ã€Œç„¡é‡ä¸€å­—ç·šã€ (Open==Close ä¸” æˆäº¤é‡ < 100å¼µ)
        consecutive_red = 0
        for i in range(len(df)-1, -1, -1):
            c = float(df['Close'].iloc[i])
            o = float(df['Open'].iloc[i])
            v = int(df['Volume'].iloc[i])
            
            # åˆ¤æ–·æ˜¯å¦ç‚ºç„¡é‡ä¸€å­—ç·š (é‡å°‘æ–¼ 100 å¼µ)
            # æ³¨æ„: FinMind volume å–®ä½ç‚ºå¼µ
            is_flat_low_vol = (c == o) and (v < 100)
            
            if c >= o and not is_flat_low_vol:  # æ”¶ç›¤ >= é–‹ç›¤ï¼Œä¸”éç„¡é‡ä¸€å­—ç·š
                consecutive_red += 1
            else:
                break
        
        # æ¢ä»¶æª¢æŸ¥
        is_breakout = current_price > prev_high
        is_above_all_ma = (
            current_price > float(today['MA5']) and
            current_price > float(today['MA10']) and
            current_price > float(today['MA20']) and
            not pd.isna(today['MA60']) and current_price > float(today['MA60'])
        )
        is_two_red_k = consecutive_red >= 2
        
        has_alert = alert_data is not None
        
        # ä¿®æ­£: å¿…é ˆç¬¦åˆçªç ´ã€å‡ç·šèˆ‡ç´…Kæ¢ä»¶ï¼Œå¦å‰‡ç›´æ¥å‰”é™¤ (ä½†å›å‚³æ¼²è·Œå¹…)
        if not (is_breakout and is_above_all_ma and is_two_red_k):
            return None, change_pct
        
        # è¨ˆç®—æ”¯æ’é»
        tech_stop = float(today['Low'])
        money_stop = current_price * 0.90
        stop_loss = max(tech_stop, money_stop)
        
        # å–å¾—ä¸­æ–‡åç¨±ã€ç”¢æ¥­ã€å¸‚å ´
        name, sector, market = get_stock_name(code)
        
        # è¨ˆç®— KD æŒ‡æ¨™ (9, 3, 3)
        k_period = 9
        d_period = 3
        
        # è¨ˆç®— RSV ä¸¦å¹³æ»‘å¾—åˆ° K, D
        df['low_9'] = df['Low'].rolling(window=k_period).min()
        df['high_9'] = df['High'].rolling(window=k_period).max()
        df['RSV'] = ((df['Close'] - df['low_9']) / (df['high_9'] - df['low_9'])) * 100
        df['RSV'] = df['RSV'].fillna(50)
        
        # K = 2/3 * å‰æ—¥K + 1/3 * RSV
        df['K'] = df['RSV'].ewm(span=3, adjust=False).mean()
        df['D'] = df['K'].ewm(span=d_period, adjust=False).mean()
        
        # è¨ˆç®— 5 æ—¥å‡é‡
        df['vol_ma5'] = df['Volume'].rolling(window=5).mean()
        
        # å–å¾— K ç·šæ•¸æ“š (æœ€è¿‘ 30 å¤©)
        ohlc_data = []
        for idx, row in df.tail(30).iterrows():
            ohlc_data.append({
                "date": idx.strftime("%Y-%m-%d"),
                "open": round(float(row['Open']), 2),
                "high": round(float(row['High']), 2),
                "low": round(float(row['Low']), 2),
                "close": round(float(row['Close']), 2),
                "volume": int(row['Volume']),
                "volMa5": int(row['vol_ma5']) if not pd.isna(row['vol_ma5']) else int(row['Volume']),
                "k": round(float(row['K']), 1) if not pd.isna(row['K']) else 50,
                "d": round(float(row['D']), 1) if not pd.isna(row['D']) else 50,
                "ma5": round(float(row['MA5']), 2) if not pd.isna(row['MA5']) else None,
                "ma10": round(float(row['MA10']), 2) if not pd.isna(row['MA10']) else None,
                "ma20": round(float(row['MA20']), 2) if not pd.isna(row['MA20']) else None
            })
        
        # å–å¾—æœ€æ–° KD å€¼
        latest_k = round(float(df['K'].iloc[-1]), 1) if not pd.isna(df['K'].iloc[-1]) else 50
        latest_d = round(float(df['D'].iloc[-1]), 1) if not pd.isna(df['D'].iloc[-1]) else 50
        
        # [NEW] è¨ˆç®—æ³¢å‹•ç‡ (åˆ¤æ–·ç®±å‹æ•´ç†)
        # å–è¿‘ 20 æ—¥æ”¶ç›¤åƒ¹è¨ˆç®—è®Šç•°ä¿‚æ•¸ (CV = std / mean)
        last_20_closes = df['Close'].tail(20)
        volatility = last_20_closes.std() / last_20_closes.mean()
        is_box_breakout = volatility < 0.05  # æ³¢å‹•ç‡å°æ–¼ 5% è¦–ç‚ºç›¤æ•´
        
        # å‹•æ…‹èª¿æ•´ Signal æ–‡å­—
        signal_text = f"ğŸ”¥ è‚¡åƒ¹å‰µ {LOOKBACK_DAYS} æ—¥æ–°é«˜"
        priority_score = 90 + consecutive_red
        
        # Tags List for Frontend
        tags = []
        if is_box_breakout:
            tags.append("ç›¤æ•´çªç ´")
            signal_text = f"ğŸš€ çªç ´ç®±å‹æ•´ç† (ä½æ³¢å‹•) + å‰µé«˜"
            priority_score += 5
            
        signal_text += f"ï¼Œå‡ç·šå¤šé ­"
        
        if has_alert:
             # å¦‚æœæ˜¯è­¦ç¤ºè‚¡ä¸”ç¬¦åˆæŠ€è¡“æ¢ä»¶ï¼ŒåŠ è¨»è­¦èª
             signal_text = f"âš ï¸ {alert_data.get('badge', 'æ³¨æ„')}è‚¡ - {signal_text}"
             priority_score += 10 # ç¨å¾®æé«˜æ¬Šé‡
        
        # è¨ˆç®—æ˜¯å¦å¯ç•¶æ²–
        cant_day_trade = False
        # 1. ä¸åœ¨ç•¶æ²–æ¸…å–®ä¸­ (åƒ…åœ¨æ¸…å–®æœ‰æŠ“åˆ°æ™‚æ‰åˆ¤æ–·)
        if allowed_day_trade_targets is not None and len(allowed_day_trade_targets) > 0:
             if code not in allowed_day_trade_targets:
                 cant_day_trade = True
        
        # 2. è™•ç½®è‚¡ (é€šå¸¸ä¸å¯ç•¶æ²–)
        if alert_data and alert_data.get('type') == 'disposition':
             cant_day_trade = True

        full_data = {
            "ticker": code,
            "name": name,
            "sector": sector,
            "market": market, # æ–°å¢å¸‚å ´åˆ¥
            "tags": tags,     # æ–°å¢æ¨™ç±¤
            "currentPrice": round(current_price, 2),
            "changePct": round(change_pct, 2),
            "canDayTrade": not cant_day_trade,
            "prevHigh": round(prev_high, 2),
            "consecutiveRed": consecutive_red,
            "stopLoss": round(stop_loss, 2),
            "k": latest_k,
            "d": latest_d,
            "volume": int(today['Volume']),
            "signal": {
                "type": "breakout", # çµ±ä¸€ç‚º breakoutï¼Œå› ç‚ºç¾åœ¨éƒ½å¿…é ˆç¬¦åˆæŠ€è¡“æ¢ä»¶
                "text": f"{signal_text}ã€‚æŠ€è¡“æ”¯æ’ä½ {round(stop_loss, 1)}",
                "priority": priority_score
            },
            "ohlc": ohlc_data,
            "alert": alert_data  # Add Alert Info (None if normal)
        }
        
        return full_data, change_pct
        
    except Exception as e:
        # éœé»˜å¿½ç•¥éŒ¯èª¤
        return None, None


def calculate_changes(previous_data: Optional[dict], current_stocks: list) -> dict:
    """
    è¨ˆç®—èˆ‡å‰ä¸€æ—¥çš„å·®ç•° (æ–°é€²ã€çºŒæ¼²ã€å‰”é™¤)
    """
    if not previous_data or 'stocks' not in previous_data:
        return {
            "new": current_stocks,
            "continued": [],
            "removed": []
        }

    # Detect if we are updating on the same day
    prev_date = previous_data.get('date', '')
    is_same_day = prev_date == datetime.now().strftime("%Y-%m-%d")
    
    prev_map = {s['ticker']: s for s in previous_data['stocks']}
    
    # If same day, we need to reconstruct "Yesterday's State" to properly calculate Today's changes
    # Yesterday's Stocks = (Today's Stocks - Today's New) + Today's Removed
    if is_same_day and 'changes' in previous_data:
        print("â„¹ï¸ æª¢æ¸¬åˆ°åŒæ—¥æ›´æ–°ï¼Œæ­£åœ¨é‡å»ºæ˜¨æ—¥ç‹€æ…‹ä»¥ç¶­æŒå·®ç•°è¨ˆç®—æº–ç¢ºæ€§...")
        current_existing_tickers = set(prev_map.keys())
        
        # 1. Provide tickers that were "New" today (so they weren't there yesterday)
        today_new_tickers = {s['ticker'] for s in previous_data['changes'].get('new', [])}
        
        # 2. Provide tickers that were "Removed" today (so they WERE there yesterday)
        today_removed_list = previous_data['changes'].get('removed', [])
        today_removed_map = {s['ticker']: s for s in today_removed_list}
        
        # Reconstruct Yesterday's set
        # Yesterday = (Current - New) U Removed
        reconstructed_prev_tickers = (current_existing_tickers - today_new_tickers) | set(today_removed_map.keys())
        
        # Rebuild prev_map for calculation
        # We need the stock objects. For 'removed', we have them. 
        # For 'continued' (Current - New), they are in prev_map.
        
        real_prev_map = {}
        for t in reconstructed_prev_tickers:
            if t in today_removed_map:
                real_prev_map[t] = today_removed_map[t]
            elif t in prev_map:
                real_prev_map[t] = prev_map[t]
                
        prev_map = real_prev_map
        print(f"   é‡å»ºå®Œæˆ: æ˜¨æ—¥å…±æœ‰ {len(prev_map)} æª”è‚¡ç¥¨")

    curr_map = {s['ticker']: s for s in current_stocks}
    
    prev_tickers = set(prev_map.keys())
    curr_tickers = set(curr_map.keys())
    
    # æ–°é€²: ä»Šæœ‰æ˜¨ç„¡
    new_tickers = curr_tickers - prev_tickers
    new_list = [curr_map[t] for t in new_tickers]
    
    # çºŒæ¼²: ä»Šæœ‰æ˜¨æœ‰
    continued_tickers = curr_tickers & prev_tickers
    continued_list = [curr_map[t] for t in continued_tickers]
    
    # å‰”é™¤: ä»Šç„¡æ˜¨æœ‰
    removed_tickers = prev_tickers - curr_tickers
    removed_list = [prev_map[t] for t in removed_tickers]
    
    return {
        "new": sorted(new_list, key=lambda x: x['ticker']),
        "continued": sorted(continued_list, key=lambda x: x['ticker']),
        "removed": sorted(removed_list, key=lambda x: x['ticker'])
    }


def update_existing_alerts():
    """åƒ…æ›´æ–°ç¾æœ‰æª”æ¡ˆä¸­çš„è­¦ç¤ºè³‡è¨Š"""
    print(f"\n=== å¸‚å ´è­¦ç¤ºæ›´æ–°æ¨¡å¼ ===")
    output_file = OUTPUT_DIR / "daily_scan_results.json"
    
    if not output_file.exists():
        print("éŒ¯èª¤ï¼šæ‰¾ä¸åˆ°æƒæçµæœæª”æ¡ˆï¼Œç„¡æ³•æ›´æ–°è­¦ç¤º")
        sys.exit(1)
        
    try:
        with open(output_file, 'r', encoding='utf-8') as f:
            data = json.load(f)
            
        market_alerts = fetch_market_alerts()
        print(f"å–å¾—å¸‚å ´è­¦ç¤ºè³‡æ–™: {len(market_alerts)} ç­†")
        
        updated_count = 0
        stocks = data.get('stocks', [])
        
        for stock in stocks:
            code = stock['ticker']
            alert_data = market_alerts.get(code)
            
            # Update alert field (even if None, to clear old alerts if they expired)
            if stock.get('alert') != alert_data:
                stock['alert'] = alert_data
                updated_count += 1
                if alert_data:
                    print(f"âš ï¸ {code} {stock['name']} æ–°å¢/æ›´æ–°è­¦ç¤º: {alert_data['badge']}")
        
        # Update timestamps
        # If quoteTime doesn't exist (legacy), use old updatedAt as quoteTime
        if 'quoteTime' not in data:
            data['quoteTime'] = data.get('updatedAt')
            
        data['alertUpdateTime'] = datetime.now().isoformat()
        data['updatedAt'] = datetime.now().isoformat() # General update time
        
        # Save
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=2)
            
        print(f"âœ… å·²æ›´æ–° {updated_count} ç­†è­¦ç¤ºç‹€æ…‹")
        print(f"è­¦ç¤ºæ›´æ–°æ™‚é–“: {data['alertUpdateTime']}")
        
        return data
        
    except Exception as e:
        print(f"æ›´æ–°è­¦ç¤ºå¤±æ•—: {e}")
        sys.exit(1)

# -----------------------------------------------
# Article Generation Integration
# -----------------------------------------------
try:
    from article_generator import generate_daily_article, save_to_json, generate_articles_index
except ModuleNotFoundError:
    from scripts.article_generator import generate_daily_article, save_to_json, generate_articles_index

def process_single_stock(code, market_alerts, allowed_day_trade_targets):
    """Worker function for parallel processing"""
    try:
        # Small delay to prevent burst rate limit
        time.sleep(0.1) 
        data, change_pct = check_livermore_criteria(code, market_alerts, allowed_day_trade_targets)
        return code, data, change_pct
    except Exception as e:
        print(f"Error processing {code}: {e}")
        return code, None, None

def main():
    """ä¸»ç¨‹å¼"""
    import argparse
    parser = argparse.ArgumentParser()
    parser.add_argument('--update-alerts', action='store_true', help='Update existing alerts only')
    parser.add_argument('--generate-article-only', action='store_true', help='Generate article from existing data only')
    args = parser.parse_args()

    # Check arguments
    if args.update_alerts:
        data = update_existing_alerts()
        
        # Merge article generation for alert updates
        try:
            print("æ­£åœ¨æ›´æ–°ç›¤å‹¢åˆ†ææ–‡ç«  (å«è­¦ç¤ºè³‡è¨Š)...")
            article = generate_daily_article(data)
            save_to_json(article)
            print("âœ… å·²æ›´æ–°æ¯æ—¥åˆ†ææ–‡ç« ä¸¦å„²å­˜")
        except Exception as e:
            print(f"âš ï¸ æ–‡ç« æ›´æ–°å¤±æ•—: {e}")
            
        return

    # Check Manual Article Trigger
    if args.generate_article_only:
        print("ğŸš€ Manual Trigger: Generating Article Only")
        output_file = OUTPUT_DIR / "daily_scan_results.json"
        
        if not output_file.exists():
            print(f"âŒ Error: {output_file} not found. Cannot generate article.")
            sys.exit(1)
            
        try:
            with open(output_file, 'r', encoding='utf-8') as f:
                data = json.load(f)
            
            article = generate_daily_article(data)
            if save_to_json(article):
                print("âœ… Manual article generation and save completed successfully.")
            else:
                print("âš ï¸ Article generated but NOT saved (check errors above).")
                sys.exit(1)
            return
        except Exception as e:
            print(f"âŒ Failed to generate article: {e}")
            sys.exit(1)

    print(f"\n=== åˆ©å¼—æ‘©çˆ¾å¼·å‹¢çªç ´æƒæ ===")
    print(f"æƒææ™‚é–“: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print(f"çªç ´å¤©æ•¸: {LOOKBACK_DAYS} æ—¥\n")
    
    # ç¢ºä¿ç›®éŒ„å­˜åœ¨
    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)
    output_file = OUTPUT_DIR / "daily_scan_results.json"
    
    # è®€å–èˆŠè³‡æ–™ (ç”¨æ–¼è¨ˆç®—å·®ç•°)
    previous_data = None
    if output_file.exists():
        try:
            with open(output_file, 'r', encoding='utf-8') as f:
                previous_data = json.load(f)
        except Exception as e:
            print(f"ç„¡æ³•è®€å–èˆŠè³‡æ–™: {e}")

    # å–å¾—è‚¡ç¥¨æ¸…å–®
    target_list = get_all_tw_targets()
    total = len(target_list)
    
    # è­¦å‘Šï¼šç„¡ Token æ™‚æƒæå¤§é‡è‚¡ç¥¨é¢¨éšª
    token = os.environ.get("FINMIND_API_TOKEN")
    
    # [NEW] Anonymous Optimization: Prioritize and Limit
    if not token and total > 600:
        print(f"âš ï¸ æœªè¨­å®š Tokenï¼Œå•Ÿç”¨ã€ŒåŒ¿åå®‰å…¨æ¨¡å¼ã€")
        print(f"   å°‡é™åˆ¶æƒæå‰ 550 æª”ç†±é–€è‚¡ç¥¨ï¼Œä»¥é¿å…è§¸ç™¼ API é™åˆ¶ (600æ¬¡/hr)ã€‚")
        
        # Load ranks to prioritize
        rank_file = OUTPUT_DIR / "market_cap_rank.json"
        ranks = {}
        if rank_file.exists():
            try:
                with open(rank_file, 'r', encoding='utf-8') as f:
                    ranks = json.load(f).get("ranks", {})
            except: pass
            
        # Sort: Ranked stocks first (low rank number), then others
        target_list.sort(key=lambda x: ranks.get(x, 99999))
        
        # Slice
        target_list = target_list[:550]
        total = len(target_list)
        print(f"   âœ… å·²ç¯©é¸å‰ {total} æª”é«˜å¸‚å€¼è‚¡ç¥¨é€²è¡Œæƒæã€‚")
    elif not token:
        print(f"âš ï¸ è­¦å‘Š: æœªè¨­å®š Tokenï¼Œä½†è‚¡ç¥¨æ•¸é‡ {total} åœ¨é™åˆ¶ç¯„åœå…§ï¼Œç¹¼çºŒåŸ·è¡Œã€‚")
    
    # å–å¾—å¸‚å ´è­¦ç¤º (è™•ç½®/æ³¨æ„)
    market_alerts = fetch_market_alerts()
    print(f"å–å¾—å¸‚å ´è­¦ç¤ºè³‡æ–™: {len(market_alerts)} ç­†")
    

    
    # å–å¾—å¯ç•¶æ²–æ¨™çš„æ¸…å–®
    allowed_day_trade_targets = fetch_allowed_day_trade_targets()
    
    results = []
    
    # å¸‚å ´å¯¬åº¦çµ±è¨ˆ (Market Breadth)
    market_stats = {
        "up": 0,
        "down": 0,
        "flat": 0,
        "total_scanned": 0
    }
    
    print(f"ğŸš€ é–‹å§‹å¹³è¡Œæƒæ (Workers: {MAX_WORKERS})...")
    start_time = time.time()
    
    with concurrent.futures.ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:
        # Submit tasks
        futures = {executor.submit(process_single_stock, code, market_alerts, allowed_day_trade_targets): code for code in target_list}
        
        completed_count = 0
        for future in concurrent.futures.as_completed(futures):
            code = futures[future]
            try:
                _, data, change_pct = future.result()
                
                completed_count += 1
                if completed_count % 10 == 0:
                    print(f"\ré€²åº¦: {completed_count}/{total} ({(completed_count/total)*100:.1f}%)", end="", flush=True)
                
                # çµ±è¨ˆå¸‚å ´æ¼²è·Œ
                if change_pct is not None:
                    market_stats["total_scanned"] += 1
                    if change_pct > 0:
                        market_stats["up"] += 1
                    elif change_pct < 0:
                        market_stats["down"] += 1
                    else:
                        market_stats["flat"] += 1
                        
                if data:
                    results.append(data)
                    
            except Exception as exc:
                print(f"\nError processing {code}: {exc}")

    elapsed = time.time() - start_time
    print(f"\n\næƒæå®Œæˆï¼è€—æ™‚: {elapsed:.2f} ç§’")
    print(f"å¸‚å ´çµ±è¨ˆ: ä¸Šæ¼² {market_stats['up']} / ä¸‹è·Œ {market_stats['down']} / å¹³ç›¤ {market_stats['flat']}")
    print(f"ç¬¦åˆæ¢ä»¶: {len(results)} æª”\n")
    

    
    # Restore sort by priority (Signal Strength)
    results.sort(key=lambda x: x['signal']['priority'], reverse=True)
    
    # è¨ˆç®—å·®ç•°
    changes = calculate_changes(previous_data, results)

    # è¼¸å‡ºçµæœ
    if results:
        print("=" * 60)
        print(f"{'ä»£è™Ÿ':<8} {'åç¨±':<10} {'ç¾åƒ¹':>8} {'é€£ç´…':>4} {'ç‹€æ…‹':<6}")
        print("=" * 60)
        
        # å»ºç«‹å¿«é€ŸæŸ¥æ‰¾ map
        new_tickers = {s['ticker'] for s in changes['new']}
        
        for r in results:
            status = "âœ¨æ–°é€²" if r['ticker'] in new_tickers else "âŸ³çºŒæ¼²"
            print(f"{r['ticker']:<8} {r['name']:<10} {r['currentPrice']:>8.2f} {r['consecutiveRed']:>4} {status:<6}")

    current_iso = datetime.now().isoformat()
    
    # æº–å‚™ JSON è¼¸å‡º
    output = {
        "date": datetime.now().strftime("%Y-%m-%d"),
        "updatedAt": current_iso,
        "quoteTime": current_iso, 
        "alertUpdateTime": current_iso,
        "scanType": "livermore_breakout",
        "criteria": {
            "lookbackDays": LOOKBACK_DAYS,
            "description": f"çªç ´ {LOOKBACK_DAYS} æ—¥æ–°é«˜ + ç«™ä¸Šæ‰€æœ‰å‡ç·š + é€£çºŒ2æ—¥ç´…K"
        },
        "stocks": results,
        "marketStats": market_stats, # æ–°å¢å¸‚å ´çµ±è¨ˆæ¬„ä½
        "summary": {
            "total": len(results),
            "buySignals": len(results),
            "counts": {
                "new": len(changes['new']),
                "continued": len(changes['continued']),
                "removed": len(changes['removed'])
            }
        },
        "changes": changes
    }
    
    # å¯«å…¥ JSON
    output_file = OUTPUT_DIR / "daily_scan_results.json"
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(output, f, ensure_ascii=False, indent=2)
    
    # [NEW] Save History JSON for Article Page
    history_dir = OUTPUT_DIR / "history"
    history_dir.mkdir(exist_ok=True)
    history_file = history_dir / f"{output['date']}.json"
    with open(history_file, 'w', encoding='utf-8') as f:
        json.dump(output, f, ensure_ascii=False, indent=2)
    print(f"âœ… History saved to {history_file}")
    
    print(f"\nâœ… å·²è¼¸å‡ºè‡³ {output_file}")

    # -----------------------------------------------
    # Auto Generate Article
    # -----------------------------------------------
    try:
        print("æ­£åœ¨ç”¢ç”Ÿç›¤å‹¢åˆ†ææ–‡ç« ...")
        article = generate_daily_article(output)
        save_to_json(article)
        print("âœ… å·²ç”¢ç”Ÿæ¯æ—¥åˆ†ææ–‡ç« ä¸¦å„²å­˜")
    except Exception as e:
        print(f"âš ï¸ æ–‡ç« ç”¢ç”Ÿå¤±æ•— (ä¸å½±éŸ¿ä¸»æµç¨‹): {e}")
    
    return output



if __name__ == "__main__":
    main()
    # Always regenerate index after main process
    try:
        generate_articles_index()
    except Exception as e:
        print(f"Index generation failed: {e}")
