#!/usr/bin/env python3
"""
åˆ©å¼—æ‘©çˆ¾å¼·å‹¢çªç ´æƒæå™¨
æ¯æ—¥ 14:30 åŸ·è¡Œï¼Œç¯©é¸ç¬¦åˆæ¢ä»¶çš„å¼·å‹¢è‚¡

ç¯©é¸æ¢ä»¶ï¼š
1. è‚¡åƒ¹ç«™ä¸Šæ‰€æœ‰å‡ç·š (MA5, MA10, MA20, MA60)
2. é€£çºŒå…©æ—¥ç´… K (æ”¶ç›¤ > é–‹ç›¤)
3. æ”¶ç›¤åƒ¹çªç ´è¿‘ N æ—¥æ–°é«˜
4. (ä¿®æ­£) è­¦ç¤º/è™•ç½®è‚¡ä¹Ÿå¿…é ˆç¬¦åˆä¸Šè¿°æŠ€è¡“æ¢ä»¶æ‰èƒ½å…¥é¸
"""
import json
import os
import sys
from datetime import datetime
from pathlib import Path

import pandas as pd

# FinMind API for Taiwan stock data (more reliable than yfinance)
from FinMind.data import DataLoader

# Initialize FinMind DataLoader (singleton)
_finmind_loader = None

def get_finmind_loader():
    """Get or create FinMind DataLoader singleton"""
    global _finmind_loader
    if _finmind_loader is None:
        _finmind_loader = DataLoader()
        token = os.environ.get("FINMIND_API_TOKEN")
        if token:
            _finmind_loader.login_by_token(api_token=token)
            print("âœ… FinMind logged in with token")
    return _finmind_loader

try:
    import twstock
    # å¼·åˆ¶æ›´æ–°è‚¡ç¥¨ä»£ç¢¼è¡¨ï¼Œç¢ºä¿æ“æœ‰æœ€æ–°ä¸Šå¸‚æ«ƒæ¸…å–®
    # twstock.__update_codes() # æ³¨æ„: é€™å¯èƒ½éœ€è¦ä¸‹è¼‰ï¼Œè‹¥ CI ç’°å¢ƒå—é™å¯èƒ½å¤±æ•—ï¼Œè¦–æƒ…æ³å•Ÿç”¨
    HAS_TWSTOCK = True
except ImportError:
    HAS_TWSTOCK = False
    print("Warning: twstock not installed, using FinMind for stock names")

import re
import requests
from datetime import timedelta

def roc_to_date(roc_str):
    """Convert ROC date string (e.g., '114/01/05') to datetime object"""
    try:
        parts = roc_str.split('/')
        year = int(parts[0]) + 1911
        return datetime(year, int(parts[1]), int(parts[2]))
    except:
        return None

def fetch_market_alerts():
    """Fetch TWSE/TPEx Warning and Disposition data with Risk Analysis"""
    alerts = {}
    history_db = {} # {code: [dates]}
    
    today = datetime.now()
    today_str = today.strftime('%Y%m%d')
    # Look back 40 days to ensure we have enough trading days for the 30-day rule
    start_str = (today - timedelta(days=40)).strftime('%Y%m%d') 
    
    # 1. Fetch TWSE Warning History (Notice)
    try:
        url = "https://www.twse.com.tw/rwd/zh/announcement/notice"
        params = {'response': 'json', 'startDate': start_str, 'endDate': today_str}
        r = requests.get(url, params=params, timeout=10)
        data = r.json()
        
        if 'data' in data:
            for item in data['data']:
                code = item[1]
                date_roc = item[5] # e.g. "114/01/30"
                reason = item[4]
                
                if code not in history_db:
                    history_db[code] = []
                history_db[code].append(date_roc)
                
                # If it's today's alert, initialize the alert object
                alert_dt = roc_to_date(date_roc)
                if alert_dt and (today - alert_dt).days <= 1:
                    alerts[code] = {
                        "type": "warning",
                        "badge": "è­¦ç¤º",
                        "color": "yellow",
                        "info": "æ³¨æ„è‚¡",
                        "detail": reason,
                        "history": []
                    }
    except Exception as e:
        print(f"Error fetching TWSE notice: {e}")

    # 2. Fetch TWSE Disposition (Punish)
    try:
        url = "https://www.twse.com.tw/rwd/zh/announcement/punish"
        params = {'response': 'json', 'startDate': start_str, 'endDate': today_str}
        r = requests.get(url, params=params, timeout=10)
        data = r.json()
        
        if 'data' in data:
            for item in data['data']:
                code = item[2]
                period_str = item[6] 
                content = item[8]
                
                if 'ï½' in period_str:
                    start_roc, end_roc = period_str.split('ï½')
                    start_dt = roc_to_date(start_roc)
                    end_dt = roc_to_date(end_roc)
                    
                    if start_dt and end_dt and start_dt <= today + timedelta(days=1) and today <= end_dt + timedelta(days=1): 
                        freq = "è™•ç½®"
                        match = re.search(r'æ¯(\S+)åˆ†é˜', content)
                        if match:
                            freq = f"{match.group(1)}åˆ†ç›¤"
                        elif "äººå·¥ç®¡åˆ¶" in content:
                            freq = "äººå·¥ç®¡åˆ¶"
                            
                        alerts[code] = {
                            "type": "disposition",
                            "badge": "è™•ç½®",
                            "color": "red",
                            "info": f"{freq} (è‡³ {end_roc})",
                            "detail": f"æœŸé–“: {period_str}\næªæ–½: {item[7]}",
                            "is_disposed": True
                        }
    except Exception as e:
        print(f"Error fetching TWSE punish: {e}")

    # 3. Fetch TPEX (OTC) Alerts
    try:
        base_url = "https://www.tpex.org.tw/openapi/v1"
        
        # 3.1 TPEX Warning History
        r = requests.get(f"{base_url}/tpex_trading_warning_information", timeout=10)
        if r.status_code == 200:
            for item in r.json():
                code = item.get('SecuritiesCompanyCode')
                date_roc = item.get('Date') # Compact "1140130"
                reason = item.get('TradingInformation', '')
                
                # Convert to slash format for consistency
                if len(date_roc) == 7:
                    fmt_date = f"{date_roc[:3]}/{date_roc[3:5]}/{date_roc[5:]}"
                else:
                    fmt_date = date_roc
                    
                if code not in history_db:
                    history_db[code] = []
                history_db[code].append(fmt_date)
                
                # Check if recent
                alert_dt = None
                if len(date_roc) == 7:
                    alert_dt = datetime(int(date_roc[:3]) + 1911, int(date_roc[3:5]), int(date_roc[5:]))
                
                if alert_dt and (today - alert_dt).days <= 1:
                    if code not in alerts:
                        alerts[code] = {
                            "type": "warning",
                            "badge": "è­¦ç¤º",
                            "color": "yellow",
                            "info": "æ³¨æ„è‚¡",
                            "detail": reason,
                            "history": []
                        }

        # 3.2 TPEX Disposition
        r = requests.get(f"{base_url}/tpex_disposal_information", timeout=10)
        if r.status_code == 200:
            for item in r.json():
                code = item.get('SecuritiesCompanyCode')
                period_str = item.get('DispositionPeriod', '')
                content = item.get('DisposalCondition', '')
                
                if '~' in period_str:
                    try:
                        start_roc, end_roc = period_str.split('~')
                        def parse_roc_compact(d_str):
                            if len(d_str) == 7:
                                return datetime(int(d_str[:3]) + 1911, int(d_str[3:5]), int(d_str[5:]))
                            return roc_to_date(d_str)
                            
                        start_dt = parse_roc_compact(start_roc)
                        end_dt = parse_roc_compact(end_roc)
                        
                        if start_dt and end_dt and start_dt <= today + timedelta(days=1) and today <= end_dt + timedelta(days=1):
                            freq = "è™•ç½®"
                            match = re.search(r'æ¯(\S+)åˆ†é˜', content)
                            if match:
                                freq = f"{match.group(1)}åˆ†ç›¤"
                            elif "äººå·¥ç®¡åˆ¶" in content:
                                freq = "äººå·¥ç®¡åˆ¶"
                                
                            alerts[code] = {
                                "type": "disposition",
                                "badge": "è™•ç½®",
                                "color": "red",
                                "info": f"{freq} (è‡³ {end_roc})",
                                "detail": f"æœŸé–“: {period_str}\næªæ–½: {content}",
                                "is_disposed": True
                            }
                    except: pass
    except Exception as e:
        print(f"Error fetching TPEX alerts: {e}")

    # 4. Risk Analysis (Calculating Disposition Proximity)
    for code, alert_obj in alerts.items():
        if alert_obj.get('type') == 'disposition':
            continue # Already disposed
            
        hist = sorted(list(set(history_db.get(code, []))), reverse=True)
        alert_obj['history'] = hist
        
        # Calculate Risk Metrics
        # Rule 1: 3 consecutive days
        consecutive = 0
        # This is tricky because we need trading days. 
        # For simplicity, we count consecutive entries in the sorted history.
        consecutive = 1
        # Check if previous days were also in history
        # (Need a calendar or just check if dates are close)
        # For now, let's just use the count in recent window.
        
        count_30 = len(hist)
        count_6 = 0
        # Approx count in last 6 trading days (approx 10 calendar days)
        cutoff_6 = today - timedelta(days=10)
        for d_str in hist:
            d_dt = roc_to_date(d_str)
            if d_dt and d_dt >= cutoff_6:
                count_6 += 1
        
        # Determine Risk Level
        risk_level = "low"
        risk_msg = ""
        
        if count_6 >= 3:
            risk_level = "high"
            risk_msg = f"è§¸ç™¼ 4/6 è™•ç½®é¢¨éšª (ç›®å‰ {count_6}/6)"
        elif count_6 >= 2:
            risk_level = "medium"
            risk_msg = f"è¿‘æœŸæ³¨æ„æ¬¡æ•¸å¢åŠ  ({count_6}/6)"
            
        if count_30 >= 10:
            risk_level = "high"
            risk_msg = f"è§¸ç™¼ 12/30 è™•ç½®é¢¨éšª (ç›®å‰ {count_30}/30)"
            
        alert_obj['risk'] = {
            "level": risk_level,
            "message": risk_msg,
            "count_6": count_6,
            "count_30": count_30
        }

    return alerts

def fetch_allowed_day_trade_targets():
    """å–å¾—æ‰€æœ‰å¯ç¾è‚¡ç•¶æ²–çš„è‚¡ç¥¨ä»£ç¢¼ (ä¸Šå¸‚+ä¸Šæ«ƒ)"""
    allowed = set()
    
    # 1. TWSE (ä¸Šå¸‚)
    try:
        # TWTB4U: ç•¶æ—¥æ²–éŠ·äº¤æ˜“æ¨™çš„åŠæˆäº¤é‡å€¼
        # è‹¥ä¸å¸¶æ—¥æœŸï¼Œé è¨­å›å‚³æœ€è¿‘äº¤æ˜“æ—¥
        url = "https://www.twse.com.tw/exchangeReport/TWTB4U?response=json"
        r = requests.get(url, timeout=10)
        data = r.json()
        if 'tables' in data:
            for t in data['tables']:
                # å°‹æ‰¾åŒ…å«æ¨™çš„æ¸…å–®çš„è¡¨æ ¼ (é€šå¸¸æ˜¯ç¬¬äºŒå€‹ï¼Œæ¬„ä½å«'è­‰åˆ¸ä»£è™Ÿ')
                if 'fields' in t and 'è­‰åˆ¸ä»£è™Ÿ' in t['fields']:
                    if len(t.get('data', [])) > 100: # ç°¡å–®æª¢æ ¸è³‡æ–™é‡
                        for row in t['data']:
                            allowed.add(row[0]) # ä»£è™Ÿ
                        print(f"å·²å–å¾—ä¸Šå¸‚ç•¶æ²–æ¨™çš„: {len(t['data'])} æª”")
    except Exception as e:
        print(f"Error fetching TWSE day trade list: {e}")

    # 2. TPEX (ä¸Šæ«ƒ)
    try:
        # TPEX éœ€è¦æŒ‡å®šæ—¥æœŸï¼Œå˜—è©¦å›æ¨æœ€è¿‘ 5 å¤©ç›´åˆ°æŠ“åˆ°è³‡æ–™
        found = False
        base_date = datetime.now()
        
        for i in range(5):
            d = base_date - timedelta(days=i)
            roc_year = d.year - 1911
            date_str = f"{roc_year}/{d.month:02d}/{d.day:02d}"
            
            url = f"https://www.tpex.org.tw/web/stock/trading/intraday_stat/intraday_trading_stat_result.php?l=zh-tw&o=json&d={date_str}"
            try:
                r = requests.get(url, timeout=5)
                data = r.json()
                if 'tables' in data:
                    for t in data['tables']:
                         if 'fields' in t and 'è­‰åˆ¸ä»£è™Ÿ' in t['fields']:
                            count = len(t.get('data', []))
                            if count > 50: # ç°¡å–®æª¢æ ¸
                                for row in t['data']:
                                    allowed.add(row[0])
                                print(f"å·²å–å¾—ä¸Šæ«ƒç•¶æ²–æ¨™çš„ ({date_str}): {count} æª”")
                                found = True
                                break
                    if found: break
            except:
                continue
                
    except Exception as e:
        print(f"Error fetching TPEX day trade list: {e}")
        
    print(f"ç¸½è¨ˆå¯ç•¶æ²–æ¨™çš„: {len(allowed)} æª”")
    return allowed

import concurrent.futures
import time
import threading

# --- è¨­å®š ---
LOOKBACK_DAYS = 20  # çªç ´å¹¾æ—¥æ–°é«˜
TEST_MODE = os.environ.get('TEST_MODE', 'true').lower() == 'true'  # GitHub Actions è¨­ç‚º false
OUTPUT_DIR = Path("frontend/public/data")
MAX_WORKERS = int(os.environ.get('MAX_WORKERS', 5)) # Parallel workers

# ... (TEST_STOCKS list remains same) ...

# ... (Helper functions remain same) ...

def process_single_stock(code, market_alerts, allowed_day_trade_targets):
    """Worker function for parallel processing"""
    try:
        # Small delay to prevent burst rate limit
        time.sleep(0.1) 
        data, change_pct = check_livermore_criteria(code, market_alerts, allowed_day_trade_targets)
        return code, data, change_pct
    except Exception as e:
        print(f"Error processing {code}: {e}")
        return code, None, None

def main():
    """ä¸»ç¨‹å¼"""
    import argparse
    parser = argparse.ArgumentParser()
    parser.add_argument('--update-alerts', action='store_true', help='Update existing alerts only')
    parser.add_argument('--generate-article-only', action='store_true', help='Generate article from existing data only')
    args = parser.parse_args()

    # ... (Alerts update logic remains same) ...
    if args.update_alerts:
        data = update_existing_alerts()
        try:
            print("æ­£åœ¨æ›´æ–°ç›¤å‹¢åˆ†ææ–‡ç«  (å«è­¦ç¤ºè³‡è¨Š)...")
            article = generate_daily_article(data)
            save_to_json(article)
            print("âœ… å·²æ›´æ–°æ¯æ—¥åˆ†ææ–‡ç« ä¸¦å„²å­˜")
        except Exception as e:
            print(f"âš ï¸ æ–‡ç« æ›´æ–°å¤±æ•—: {e}")
        return

    # ... (Generate article logic remains same) ...
    if args.generate_article_only:
        # ... (implementation same as before) ...
        print("ğŸš€ Manual Trigger: Generating Article Only")
        output_file = OUTPUT_DIR / "daily_scan_results.json"
        if not output_file.exists():
            print(f"âŒ Error: {output_file} not found. Cannot generate article.")
            sys.exit(1)
        try:
            with open(output_file, 'r', encoding='utf-8') as f:
                data = json.load(f)
            article = generate_daily_article(data)
            if save_to_json(article):
                print("âœ… Manual article generation and save completed successfully.")
            else:
                print("âš ï¸ Article generated but NOT saved (check errors above).")
                sys.exit(1)
            return
        except Exception as e:
            print(f"âŒ Failed to generate article: {e}")
            sys.exit(1)

    print(f"\n=== åˆ©å¼—æ‘©çˆ¾å¼·å‹¢çªç ´æƒæ ===")
    print(f"æƒææ™‚é–“: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print(f"çªç ´å¤©æ•¸: {LOOKBACK_DAYS} æ—¥\n")
    
    # ç¢ºä¿ç›®éŒ„å­˜åœ¨
    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)
    output_file = OUTPUT_DIR / "daily_scan_results.json"
    
    # è®€å–èˆŠè³‡æ–™ (ç”¨æ–¼è¨ˆç®—å·®ç•°)
    previous_data = None
    if output_file.exists():
        try:
            with open(output_file, 'r', encoding='utf-8') as f:
                previous_data = json.load(f)
        except Exception as e:
            print(f"ç„¡æ³•è®€å–èˆŠè³‡æ–™: {e}")

    # å–å¾—è‚¡ç¥¨æ¸…å–®
    target_list = get_all_tw_targets()
    total = len(target_list)
    
    # è­¦å‘Šï¼šç„¡ Token æ™‚æƒæå¤§é‡è‚¡ç¥¨é¢¨éšª
    token = os.environ.get("FINMIND_API_TOKEN")
    if not token and total > 600:
        print(f"âš ï¸ è­¦å‘Š: æœªè¨­å®š FINMIND_API_TOKENï¼Œæƒæ {total} æª”è‚¡ç¥¨å¯èƒ½æœƒè§¸ç™¼ API é™åˆ¶ (600æ¬¡/hr)ã€‚")
        print("   å»ºè­°è¨­å®š Token ä»¥ç²å¾— 3000æ¬¡/hr é¡åº¦ï¼Œæˆ–åƒ…ä½¿ç”¨æ¸¬è©¦æ¨¡å¼ã€‚")
    
    # å–å¾—å¸‚å ´è­¦ç¤º (è™•ç½®/æ³¨æ„)
    market_alerts = fetch_market_alerts()
    print(f"å–å¾—å¸‚å ´è­¦ç¤ºè³‡æ–™: {len(market_alerts)} ç­†")
    
    # å–å¾—å¯ç•¶æ²–æ¨™çš„æ¸…å–®
    allowed_day_trade_targets = fetch_allowed_day_trade_targets()
    
    results = []
    
    # å¸‚å ´å¯¬åº¦çµ±è¨ˆ (Market Breadth)
    market_stats = {
        "up": 0,
        "down": 0,
        "flat": 0,
        "total_scanned": 0
    }
    
    print(f"ğŸš€ é–‹å§‹å¹³è¡Œæƒæ (Workers: {MAX_WORKERS})...")
    start_time = time.time()
    
    with concurrent.futures.ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:
        # Submit tasks
        futures = {executor.submit(process_single_stock, code, market_alerts, allowed_day_trade_targets): code for code in target_list}
        
        completed_count = 0
        for future in concurrent.futures.as_completed(futures):
            code = futures[future]
            try:
                _, data, change_pct = future.result()
                
                completed_count += 1
                if completed_count % 10 == 0:
                    print(f"\ré€²åº¦: {completed_count}/{total} ({(completed_count/total)*100:.1f}%)", end="", flush=True)
                
                # çµ±è¨ˆå¸‚å ´æ¼²è·Œ
                if change_pct is not None:
                    market_stats["total_scanned"] += 1
                    if change_pct > 0:
                        market_stats["up"] += 1
                    elif change_pct < 0:
                        market_stats["down"] += 1
                    else:
                        market_stats["flat"] += 1
                        
                if data:
                    results.append(data)
                    
            except Exception as exc:
                print(f"\nError processing {code}: {exc}")

    elapsed = time.time() - start_time
    print(f"\n\næƒæå®Œæˆï¼è€—æ™‚: {elapsed:.2f} ç§’")
    print(f"å¸‚å ´çµ±è¨ˆ: ä¸Šæ¼² {market_stats['up']} / ä¸‹è·Œ {market_stats['down']} / å¹³ç›¤ {market_stats['flat']}")
    print(f"ç¬¦åˆæ¢ä»¶: {len(results)} æª”\n")
    
    # æŒ‰é€£ç´…å¤©æ•¸æ’åº (è¶Šå¤šè¶Šå¼·)
    results.sort(key=lambda x: x['signal']['priority'], reverse=True)
    
    # è¨ˆç®—å·®ç•°
    changes = calculate_changes(previous_data, results)

    # è¼¸å‡ºçµæœ
    if results:
        print("=" * 60)
        print(f"{'ä»£è™Ÿ':<8} {'åç¨±':<10} {'ç¾åƒ¹':>8} {'é€£ç´…':>4} {'ç‹€æ…‹':<6}")
        print("=" * 60)
        
        # å»ºç«‹å¿«é€ŸæŸ¥æ‰¾ map
        new_tickers = {s['ticker'] for s in changes['new']}
        
        for r in results:
            status = "âœ¨æ–°é€²" if r['ticker'] in new_tickers else "âŸ³çºŒæ¼²"
            print(f"{r['ticker']:<8} {r['name']:<10} {r['currentPrice']:>8.2f} {r['consecutiveRed']:>4} {status:<6}")

    current_iso = datetime.now().isoformat()
    
    # æº–å‚™ JSON è¼¸å‡º
    output = {
        "date": datetime.now().strftime("%Y-%m-%d"),
        "updatedAt": current_iso,
        "quoteTime": current_iso, 
        "alertUpdateTime": current_iso,
        "scanType": "livermore_breakout",
        "criteria": {
            "lookbackDays": LOOKBACK_DAYS,
            "description": f"çªç ´ {LOOKBACK_DAYS} æ—¥æ–°é«˜ + ç«™ä¸Šæ‰€æœ‰å‡ç·š + é€£çºŒ2æ—¥ç´…K"
        },
        "stocks": results,
        "marketStats": market_stats, # æ–°å¢å¸‚å ´çµ±è¨ˆæ¬„ä½
        "summary": {
            "total": len(results),
            "buySignals": len(results),
            "counts": {
                "new": len(changes['new']),
                "continued": len(changes['continued']),
                "removed": len(changes['removed'])
            }
        },
        "changes": changes
    }
    
    # å¯«å…¥ JSON
    output_file = OUTPUT_DIR / "daily_scan_results.json"
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(output, f, ensure_ascii=False, indent=2)
    
    # [NEW] Save History JSON for Article Page
    history_dir = OUTPUT_DIR / "history"
    history_dir.mkdir(exist_ok=True)
    history_file = history_dir / f"{output['date']}.json"
    with open(history_file, 'w', encoding='utf-8') as f:
        json.dump(output, f, ensure_ascii=False, indent=2)
    print(f"âœ… History saved to {history_file}")
    
    print(f"\nâœ… å·²è¼¸å‡ºè‡³ {output_file}")

    # -----------------------------------------------
    # Auto Generate Article
    # -----------------------------------------------
    try:
        print("æ­£åœ¨ç”¢ç”Ÿç›¤å‹¢åˆ†ææ–‡ç« ...")
        article = generate_daily_article(output)
        save_to_json(article)
        print("âœ… å·²ç”¢ç”Ÿæ¯æ—¥åˆ†ææ–‡ç« ä¸¦å„²å­˜")
    except Exception as e:
        print(f"âš ï¸ æ–‡ç« ç”¢ç”Ÿå¤±æ•— (ä¸å½±éŸ¿ä¸»æµç¨‹): {e}")
    
    return output



def generate_articles_index():
    """Appends new articles to the existing index JSON file."""
    articles_dir = OUTPUT_DIR / "articles"
    index_file = OUTPUT_DIR / "articles_index.json"
    
    if not articles_dir.exists():
        print("âš ï¸ No articles directory found.")
        return

    # 1. Try to load existing index from data branch (GitHub)
    existing_index = []
    try:
        import urllib.request
        url = "https://raw.githubusercontent.com/jet23058/TrendGuard/data/articles_index.json"
        with urllib.request.urlopen(url, timeout=10) as response:
            existing_index = json.loads(response.read().decode('utf-8'))
            print(f"ğŸ“¥ Loaded existing index with {len(existing_index)} articles")
    except Exception as e:
        print(f"âš ï¸ Could not fetch existing index (will create new): {e}")
    
    # 2. Build a set of existing dates for deduplication
    existing_dates = {item['date'] for item in existing_index}
    
    # 3. Scan local articles directory for new articles
    new_articles = []
    for file_path in sorted(articles_dir.glob("*.json"), reverse=True):
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                data = json.load(f)
                article_date = data.get("date", file_path.stem)
                
                # Skip if already in index
                if article_date in existing_dates:
                    continue
                    
                summary_data = {
                    "date": article_date,
                    "title": data.get("title", "ç„¡æ¨™é¡Œ"),
                    "isAiGenerated": data.get("isAiGenerated", False),
                    "preview": data.get("content", "")[:100].replace('#', '').strip() + "..." 
                }
                new_articles.append(summary_data)
                print(f"â• Adding new article: {article_date}")
        except Exception as e:
            print(f"âš ï¸ Failed to read article {file_path.name}: {e}")

    # 4. Merge and sort (newest first)
    merged_index = new_articles + existing_index
    merged_index.sort(key=lambda x: x['date'], reverse=True)
    
    # 5. Write updated index file
    try:
        with open(index_file, 'w', encoding='utf-8') as f:
            json.dump(merged_index, f, ensure_ascii=False, indent=2)
        print(f"âœ… Articles index updated: {index_file} ({len(merged_index)} total articles)")
    except Exception as e:
        print(f"âŒ Failed to write articles index: {e}")


if __name__ == "__main__":
    main()
    # Always regenerate index after main process
    generate_articles_index()

